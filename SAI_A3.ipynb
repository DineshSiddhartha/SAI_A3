{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DineshSiddhartha/SAI_A3/blob/main/SAI_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TajuHlYQQXX",
        "outputId": "734f861d-6f57-497d-c6c8-a4c49dab9269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's kappa: 0.8363343684230562\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Loading annotations from JSON files\n",
        "def load_annotations(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    annotations = []\n",
        "    #extracting annotations from json files\n",
        "    for item in data:\n",
        "        results = item.get(\"annotations\", [{}])[0].get(\"result\", [])\n",
        "        sentence_labels = [\n",
        "            res[\"value\"][\"labels\"][0] for res in results\n",
        "        ]\n",
        "        annotations.append(sentence_labels)\n",
        "    return annotations\n",
        "\n",
        "# Flattening annotations\n",
        "def flatten_annotations(annotations):\n",
        "    return [label for sentence in annotations for label in sentence]\n",
        "\n",
        "file_a1 = \"annotator1.json\"\n",
        "file_a2 = \"annotator2.json\"\n",
        "\n",
        "# Loading json files and flattening annotations\n",
        "annotations_a1 = flatten_annotations(load_annotations(file_a1))\n",
        "annotations_a2 = flatten_annotations(load_annotations(file_a2))\n",
        "\n",
        "# Calculating Cohen's kappa\n",
        "kappa_score = cohen_kappa_score(annotations_a1, annotations_a2)\n",
        "print(f\"Cohen's kappa: {kappa_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cohen's kappa is 0.836, indicating a good level of agreement. However, the value is not 1 due to some challenges in understanding parts of speech. There was uncertainty in classifying words like \"number\" as either an adjective or a number and \"thousand\" as either a noun or a number. These ambiguities contributed to differences in annotations and slightly reduced the agreement."
      ],
      "metadata": {
        "id": "6DQYyOWrn5LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "\n",
        "# Loading JSON files\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Aligning annotations from all annotators\n",
        "def align_annotations(annotator_files):\n",
        "    aligned_data = {}\n",
        "    for file_path in annotator_files:\n",
        "        for item in load_json(file_path):\n",
        "            img_id = item['id']\n",
        "            label = (\n",
        "                item.get('annotations', [{}])[0]\n",
        "                .get('result', [{}])[0]\n",
        "                .get('value', {})\n",
        "                .get('choices', [None])[0]\n",
        "            )\n",
        "            if label:\n",
        "                aligned_data.setdefault(img_id, []).append(label)\n",
        "    return aligned_data\n",
        "\n",
        "# Preparing the Fleiss' Kappa input matrix\n",
        "def prepare_fleiss_kappa_matrix(aligned_data, categories):\n",
        "    matrix = []\n",
        "    for img_id, annotations in aligned_data.items():\n",
        "        row = [0] * len(categories)  # Initializing counts for each category\n",
        "        for label in annotations:\n",
        "            if label in categories:  # Increment counts for the corresponding category\n",
        "                row[categories.index(label)] += 1\n",
        "        matrix.append(row)\n",
        "    return np.array(matrix)\n",
        "\n",
        "# Annotator files\n",
        "annotator_files = ['a1.json', 'a2.json', 'a3.json']\n",
        "\n",
        "# Aligning annotations\n",
        "aligned_data = align_annotations(annotator_files)\n",
        "\n",
        "# Defining categories for labels\n",
        "categories = [\"Trucks\", \"No Trucks\"]\n",
        "\n",
        "# Preparing matrix for Fleiss' Kappa calculation\n",
        "matrix = prepare_fleiss_kappa_matrix(aligned_data, categories)\n",
        "kappa = fleiss_kappa(matrix)\n",
        "print(f\"Fleiss' Kappa: {kappa}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwHA9M1fiIjz",
        "outputId": "c1aeadd8-a0b9-4dbf-e5af-4a1b35330a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fleiss' Kappa: 0.7884841363102228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fleiss' kappa is 0.788, which shows good agreement among the annotators. However, the value is not 1 because some images had trucks that blended into the background, making them hard to see. Proper identification required zooming in and closely examining the images. Without this careful observation, trucks were easy to miss at a quick glance. Additionally, some vehicles might have been mistaken for trucks, even though they were not. These factors caused differences in the annotations and reduced the overall agreement."
      ],
      "metadata": {
        "id": "cOt1PXnMoej6"
      }
    }
  ]
}